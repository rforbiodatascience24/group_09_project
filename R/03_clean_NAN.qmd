---
title: "03_clean_NAN"
format: html
editor: visual
---

# Load the dataset

```{r}
suppressMessages(library("tidyverse"))
```

```{r}
# verify that the dataset can be loaded successfully
data_path <- "../data/02_dat_load.csv"
dataset <- read_csv(data_path,na = c( "NA","nd","-"), show_col_types = FALSE)
head(dataset)
```

# The history of the dataset

To do a efficient cleaning we need to understand the format of our data and what it represents, so let's investigate the dataset origins.

The original paper is interested in comparing the microbial populations of patients both healthy and affected by various diseases with the aim of discovering disease biomarkers and predicting of health conditions. To do so they collected the data, which are the results of shot-gun genome sequencing of the samples, from 8 different metagenomics studies (the only one available at the time of the study (2016)), which cover: liver cirrhosis, colorectal cancer, inflammatory bowel disease, obesity and Type 2 diabetes. Segata et. al declared a total of 2571 samples before their preprocessing and 2424 after.

The sequencing data was analysed with a software called MetaPhlAn2 which outputs the species-level relative abundances and presence of strain-specific markers. Based on these Segata et. al. performed machine learning studies which are outside of the scope of these analysis. As we have no functional way of opening the output of MetaPhlAn2, we cannot consult this data.

In 2020 the output a group of people (outside the SegataLab) transposed the dataset into CSV and published it on kaggle, which is where we found it.

Given the history of the dataset, it is normal and comprehensible that understanding it can be quite a challenge.

# Making sense of the dataset

```{r}
dim(dataset)
```

Starting really really simple... the dataset has 3610 rows / samples and 3513 features. We immediately see that the number of samples is larger (by 1039) that the 2571 declared in the paper.

```{r}
dataset |> 
  distinct() |> 
  nrow()
```

We can see that all rows are unique, so the hypothesis of duplicated rows is wrong.

```{r}
#Find the sampel ID's that are there more than ones and put it in to a vector
duplicates_samples <- dataset |>
  group_by(sampleID) |> 
  summarise(n_of_samples = n()) |> 
  filter(n_of_samples > 1) |> 
  pull(sampleID)

#Now for each sub dataset we check how menny duplicat of sampel ID they have dupplicat off, we see only the studyes that have dupplicats 
dataset |>
  group_by(dataset_name) |>
  summarise(
    total = n(),
    duplicates = sum(sampleID %in% duplicates_samples)
  ) |> 
  filter(duplicates > 0)
```

```{r}
#the names of dupplica sample 
print(duplicates_samples)
print(length(duplicates_samples))

```

What we DO instead see is that there are samples id which are duplicated (243 duplicates), and these are contained in the datasets found above.

This is because som of the sampelID are dupplicat more than one time

```{r}
#Find the subject ID's that are there more than ones and put it in to a vector
duplicates_subjects <- dataset |>
  group_by(subjectID) |> 
  summarise(n_of_samples = n()) |> 
  filter(n_of_samples > 1) |> 
  pull(subjectID)

#then for each dataset we see if som of the "person" are in the study more then ones
dataset |>
  group_by(dataset_name) |>
  summarise(
    total = n(),
    duplicates = sum(subjectID %in% duplicates_subjects),
    duplicates_ratio = round(duplicates / total, 3)
  ) |> 
  filter(duplicates > 0)
```

The subjectID are also duplicated (1805 duplicates). (this can men that one perseon can have patisapate more ones but we woud need to test if this is is the same value for the hole study.

```{r}
dataset |> 
  group_by(dataset_name) |> 
  summarise(n_samples = n())
```

### Where do the surplus of samples come from?

Having this information, now we can try to localize which datasets may have not been part of the original sample counts (these could have been added in later stages to the full dataset).

Here we have the first big obstacle to understanding the dataset... despite the original paper only citing 8 papers, in the dataset we have 18, and, even though, we can trace a link between the two lists for some this is really difficult. By examining the number of samples declared in the paper (and keeping in mind that some samples were discarded during preprocessing) and the ones observed we can hypothesise the following:

The fist is the value in the paper

-   WT2D: 145 vs 145 –\> perfect match
-   Segre_Human_Skin: 291 vs 291 –\> perfect match
-   Chatelier_gut_obesity: 292 vs 278 –\> acceptable due to preproccesing
-   metahit: 124 vs 110 –\> acceptable due to preproccesing
-   Quin_gut_liver_cirrhosis: 327 vs 232 –\> acceptable due to preproccesing
-   Zeller_fecal_colorectal_cancer: 156 vs 134 –\> acceptable due to preproccesing
-   hmp / hmpii: mentioned in the paper, but number of samples unspecified we have 762 + 219
-   t2dmeta_long / short: 344 vs 363 –\> more samples than expected, possible duplicates???
-   Loman2013_EcoliOutbreak_DNA_HiSeq / Miseq: never mentioned
-   Neilsen_genome_assembly: never mentioned
-   doyle_bt2: never mentioned
-   Psoriasis_2014: never mentioned
-   Candela_Africa: never mentioned in the paper
-   Tito_subsistence_gut: never mentioned
-   VerticalTransmissionPilot: never mentioned

```{r}
mentioned <- c("Chatelier_gut_obesity", "Quin_gut_liver_cirrhosis", "Segre_Human_Skin", "WT2D", "Zeller_fecal_colorectal_cancer", "hmp", "hmpii", "metahit", "t2dmeta_long", "t2dmeta_short")

val <- dataset |> 
  filter(dataset_name %in% mentioned) |>
  nrow()

val-2424
```

By only considering the dataset mentioned in the paper the number of examples is still higher than the declared 2424 (by 110 samples). So there must be some duplicates between the mentioned datasets.

```{r}
#We need to updata our list of  dupplicat sampel ID's after we remove som of the studyes

#Find the sampel ID's that are there more than ones and put it in to a vector
duplicates_samples_update <- dataset |>
  filter(dataset_name %in% mentioned) |>
  group_by(sampleID) |> 
  summarise(n_of_samples = n()) |> 
  filter(n_of_samples > 1) |> 
  pull(sampleID)
```

```{r}
#We take the datt, and for the studys in the paper we look for the dupplicats

#We se her that there is dupplicets
dataset |>
  filter(dataset_name %in% mentioned) |>
  group_by(dataset_name) |>
  summarise(
    total = n(),
    duplicates = sum(sampleID %in% duplicates_samples_update),
    uniques = n_distinct(sampleID)
  ) |> 
  filter(duplicates > 0)

#Her we study if of the 2 studyes that show duppicats have duplicats if we considere tham alone
dataset |>
  filter(dataset_name == "Chatelier_gut_obesity") |>
  group_by(sampleID) |>
  summarise(n_samples = n()) |>
  filter(n_samples > 1)

dataset |>
  filter(dataset_name == "metahit") |>
  group_by(sampleID) |>
  summarise(n_samples = n()) |>
  filter(n_samples > 1)
```

These two are the only mentioned dataset containing duplicates, but we can also see that they do not have duplicates when taken singularly, let's try to look a bit more into them.

```{r}
dataset |>
  filter(dataset_name == "metahit" | dataset_name == "Chatelier_gut_obesity") |>
  group_by(sampleID) |>
  summarise(n_samples = n()) |> 
  filter(n_samples > 1) |> 
  nrow()
```

Here we have found a problem, the combination the same sampleID should in theory be unique for each row but it is not. We can see that 71 sampleIDs are shared between these two datasets.

Unfortunatly this is not enough to cover the 110 surplus we observed and also constitutes the final road block which does not allow us to be able to pin down what were the original 2424 samples declared by Segata et. al.

### Conclusions

There are a few paths that if we where forced to use the full dataset we could think about following:

-   Considered that Segata et. al. have made public all the python code they used for the original dataset selection, preprocessing, we could run the code to get an insight on where the duplication occurs.
-   We could examine the data made available by Segata et. al. to find out which was the state of the dataset when published, problem being that the format is that of an MethPhlAn2 output.
-   Ask the publisher on kaggle of how the dataset was converted from MethPhlAn2 to CSV data.

All of these are viable options, but they exceed the aim of the project. So we are going to settle for a less satisfying but still useful option, that being limiting ourself to study only a subset of the data.

# Our dataset

We are going to focus on the WT2D dataset, which is one of the only two having a perfect match in the number of samples with those declared in the original paper, these most likely prevents us to be impacted by the effects of the preprocessing or anything that could have gone wrong during the conversion to CSV.

```{r}
dataset <- dataset |> 
  filter(dataset_name == "WT2D")
```

## Cleaning

```{r}
dim(dataset)
```

This dataset has 145 samples which are divided as following:

```{r}
dataset |> 
  group_by(disease) |> 
  summarise(n_of_samples = n())
```

The NAs correspond to the subjects described as having "normal glucose tolerance people" in the paper, which are therefore healthy.

```{r}
dataset <- dataset |> 
  mutate(disease = replace_na(disease, "healthy"))
```

A preliminary look at the names of the columns/features of the dataset (together with what can be read by quickly looking at the paper) allows us to understand a couple of things:

```{r}
dataset |> 
  select(-starts_with("k__")) |> 
  ncol()

dataset |> 
  select(starts_with("k__")) |> 
  ncol()
```

-   There are large number of metadata columns (1-211), probably due to the many different studies not sharing the same way of logging data (ex. different column names for the same feature).

-   The remaining columns (3302) which all start with k\_\_ are the relative abundance of the specific bacteria species specified in the name of the column in the specific sample.

### Remove useless columns

We are going to work separately on the metadata columns and on the data columns

```{r}
metadata <- dataset |> 
  select(-starts_with("k__"))

data <- dataset |>
  select(starts_with("k__"))

rm(dataset)
```

Get how many NAs are in each column and plot the distribution as histogram

```{r}
na_counts <- metadata %>%
  summarise(across(everything(), ~sum(is.na(.)))) |> 
  pivot_longer(everything(), names_to = "column_name", values_to = "na_count")

na_counts |> 
  ggplot(aes(x = na_count)) +
  geom_histogram(binwidth = 1)
```

We can see that the vast majority of metadata columns completely empty, so we can remove them. We also remove the il-1 column, as it only contain NAs or FALSE.

```{r}
columns_to_drop <- na_counts |> 
  filter(na_count > 100) |> 
  pull(column_name)

metadata <- metadata |> 
  select(-columns_to_drop)

metadata <- metadata |> 
  select(-`il-1`)
```

Now we remove all the colums that contain the same value across all the rows, as they do not provide any information.

```{r}
metadata <- metadata |>
  select(where(~ n_distinct(.) > 1))
```

We can also see that for each sample the sampleID is equal to the capitalized version of the subjectID, thus making the two columns redundant.

The metadata is now cleaned, the feature remaining all have information that can be useful for downstream analysis.

```{r}
metadata |> 
  mutate(redundant = sampleID == str_to_upper(subjectID)) |> 
  select(redundant) |> 
  unique()

metadata <- metadata |> 
  select(-subjectID)
```

Out of the total 3032 data columns having the relative abundances of the specified species, we can remove those not including any data for our samples (i.e. sum of their values == 0)

```{r}
data <- data |> 
  select(where(~sum(.) != 0))
```

For the remaining we count how many non-zero values are present.

```{r}
non_zero_values <- data |>
  summarise(across(everything(), ~ sum(. != 0))) |> 
  pivot_longer(everything(), names_to = "column_name", values_to = "non_zero_values")

non_zero_values |> 
  ggplot(aes(x = non_zero_values)) +
  geom_histogram(binwidth = 1)
```

This tells us that many of the columns are really sparse. But this could be simply due to the many species observed not being present in many samples.

Now the data columns are also cleaned. So we merge the two dataset back together (In both cleaning we did not change the number of rows so we can blindly merge the dataset side by side).

```{r}
full_dataset <- bind_cols(metadata, data)
```

Save the full dataset

```{r}
target_path <- "../data"
# save the dataset in the data fodler
write_csv(full_dataset, str_c(target_path, "/03_dat_clean.csv"))
```
