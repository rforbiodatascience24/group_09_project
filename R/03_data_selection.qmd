---
title: "03_data_selection"
author: "Andreis Marco"
format: html
editor: visual
---

### The history of the dataset

To do an efficient cleaning we need to understand the format of our data and what it represents, so let's investigate the dataset origins.

The original paper is interested in comparing the microbial populations of patients both healthy and affected by various diseases with the aim of discovering disease biomarkers and predicting of health conditions. To do so they collected the data, which are the results of shot-gun genome sequencing of the samples, from 8 different metagenomics studies (the only one available at the time of the study (2016)), which cover: liver cirrhosis, colorectal cancer, inflammatory bowel disease, obesity and Type 2 diabetes. Segata et. al declared a total of 2571 samples before their preprocessing and 2424 after.

The sequencing data was analysed with a software called MetaPhlAn2 which outputs the species-level relative abundances and presence of strain-specific markers. Based on these Segata et. al. performed machine learning studies which are outside of the scope of these analysis. As we have no functional way of opening the output of MetaPhlAn2, we cannot consult this data.

In 2020 the output a group of people (outside the SegataLab) transposed the dataset into CSV and published it on kaggle, which is where we found it.

### Load the dataset

```{r}
library("tidyverse")
library("here")
```

```{r}
# verify that the dataset can be loaded successfully
data_path <- here("data/02_dat_load.csv")
dataset <- read_csv(data_path,
                    show_col_types = FALSE)
```

### Verify that the dataset reflects the paper description

```{r}
dim(dataset)

dataset |> 
  distinct() |> 
  dim()
```

Starting really really simple... the dataset has 3610 rows / samples and 3513 features. We immediately see that the number of samples is larger (by 1039) that the 2571 declared in the paper.

We can see that all rows are different, so where do these additional samples come from?

```{r}
dataset |> 
  group_by(dataset_name) |> 
  summarise(n_samples = n())
```

We can trace some of these datasets to the ones mentioned in the paper, but for some this is not so easy.

The following are snippets of the original paper where the used dataset are described:

"We initially considered a total of 2571 publicly available metagenomic samples (from eight main studies/datasets) that were reduced to 2424 after pre-processing and curation (see next sections)."

"Cirrhosis included 123 patients affected by liver cirrhosis and 114 healthy controls. Colorectal consisted of a total of 156 samples, 53 of which were affected by colorectal cancer. IBD represented the first available large metagenomic dataset and includes 124 individuals, 25 were affected by inflammatory bowel disease (IBD). Obesity included 123 non-obese and 169 obese individuals. Two distinct studies were instead related to the alteration of the microbiome in subjects with type 2 diabetes (T2D). In the T2D dataset, 170 Chinese T2D patients and 174 non-diabetic controls were present. The WT2D focused on European women and included 53 T2D patients, 49 impaired glucose tolerance individuals and 43 normal glucose tolerance people."

"HMP included samples collected from five major body sites (i.e., gastrointestinal tract, nasal cavity, oral cavity, skin, and urogenital tract)."

"Finally, skin was composed by 291 samples acquired from several different skin sites \[[36](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004977#pcbi.1004977.ref036)\]."

We traced the mentioned datasets with those present in ours based on the name giving in Table 1 of the original paper.

```{r}
datasets <- c("liver_cirrhosis", "colorectal_cancer", "inflamatory_bowel_disease", "obesity","T2D", "WT2D","Human Microbiome Project", "Skin")

equivalent <- c("Quin_gut_liver_cirrhosis", "Zeller_fecal_colorectal_cancer", "metahit", "Chatelier_gut_obesity", "t2dmeta_long/short", "WT2D", "hmp/hmpii", "segre_Human_Skin")

declared_before_processing <- c(237, 156, 124, 292, 344, 145, 981, 291)
declared_after_processing <- c(232, 121, 110, 253, 344, 96, 981, 287)
found <- c(232, 134, 110, 278, 363, 145, 981, 291)

mentioned_vs_found <- tibble(datasets,
                             equivalent,
                             declared_before_processing,
                             declared_after_processing,
                             found)
mentioned_vs_found

mentioned_vs_found |> 
  pull(declared_after_processing) |> 
  sum()

mentioned_vs_found |> 
  pull(found) |> 
  sum()
```

The numbers of samples that we have for each dataset is equal to those obtained after the processing or intermediate between those and the total number before processing.

The only exception is for t2dmeta_long/short, for which we have more samples than expected, we are going to investigate it more during cleaning.

We do not concern ourself with replicating the processing, and will proceed with the dataset as it is, though still not considering the datasets not mentioned as the source is not clear.

```{r}
traced_datasets <- c("Quin_gut_liver_cirrhosis", "Zeller_fecal_colorectal_cancer", "metahit", "Chatelier_gut_obesity", "t2dmeta_long", "t2dmeta_short", "WT2D", "hmp", "hmpii", "segre_Human_Skin")

selected_data <- dataset |> 
  filter(dataset_name %in% traced_datasets)
```

### Save the dataset

```{r}
target_dir <- "../data"
write_csv(selected_data,
          str_c(target_dir,
                "/03_dat_selected.csv"))
```
